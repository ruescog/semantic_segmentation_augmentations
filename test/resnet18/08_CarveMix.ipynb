{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9fe9d8-746e-42f0-8f02-b9d296adaa2d",
   "metadata": {},
   "source": [
    "In this group of notebook we will test the methods implemented in the library to check if they can improve the results of the models.\n",
    "\n",
    "In order to be as impartial as possible, we will use a KFold evaluation for each combination, with a k-value of five.\n",
    "\n",
    "In this particular notebook, we will train the model using the `CarveMix` technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3700f-949f-45e5-93b1-a51ff1eab5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruescog/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback import *\n",
    "\n",
    "from semantic_segmentation_augmentations.holemakerroi import HoleMakerROI\n",
    "from semantic_segmentation_augmentations.carvemix import CarveMix\n",
    "\n",
    "from vision_models_evaluation.core import evaluate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac4f38-8a59-4174-ba46-0b7148e5f916",
   "metadata": {},
   "source": [
    "Then, we prepare the scenario: we are going to use a grape vine dataset where the semantic problem to tackle is to segmentate the RGB images into wood, leaves, grape and pole classes.\n",
    "\n",
    "Finally, we define here the mapping functions and the masks transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbdfc5-c782-423a-bfef-bb84e00c9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('dataset/')\n",
    "path_images = path/\"Images\"\n",
    "path_labels = path/\"Labels\"\n",
    "\n",
    "def get_y_fn (x):\n",
    "    return Path(str(x).replace(\"Images\",\"Labels\").replace(\"color\",\"gt\").replace(\".jpg\",\".png\"))\n",
    "\n",
    "codes = np.loadtxt(path/'codesAll.txt', dtype=str)\n",
    "\n",
    "def ParentSplitter(x):\n",
    "    return Path(x).parent.name==test_name\n",
    "\n",
    "from albumentations import (\n",
    "  Compose,\n",
    "  OneOf,\n",
    "  ElasticTransform,\n",
    "  GridDistortion, \n",
    "  OpticalDistortion,\n",
    "  HorizontalFlip,\n",
    "  Rotate,\n",
    "  Transpose,\n",
    "  CLAHE,\n",
    "  ShiftScaleRotate\n",
    ")\n",
    "\n",
    "class SegmentationAlbumentationsTransform(ItemTransform):\n",
    "    split_idx = 0\n",
    "\n",
    "    def __init__(self, aug): \n",
    "        self.aug = aug\n",
    "\n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "        aug = self.aug(image=np.array(img), mask=np.array(mask))\n",
    "        return PILImage.create(aug[\"image\"]), PILMask.create(aug[\"mask\"])\n",
    "\n",
    "transforms=Compose([HorizontalFlip(p=0.5),\n",
    "                    Rotate(p=0.40,limit=10),GridDistortion()\n",
    "                    ],p=1)\n",
    "\n",
    "transformPipeline=SegmentationAlbumentationsTransform(transforms)\n",
    "\n",
    "class TargetMaskConvertTransform(ItemTransform):\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    def encodes(self, x):\n",
    "        img,mask = x\n",
    "\n",
    "        #Convert to array\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # background = 0, leaves = 1, pole = 74 o 76, wood = 25 o 29, grape = 255\n",
    "        mask[mask == 255] = 1 # grape\n",
    "        mask[mask == 150] = 2 # leaves\n",
    "        mask[mask == 76] = 3 ; mask[mask == 74] = 3 # pole\n",
    "        mask[mask == 29] = 4 ; mask[mask == 25] = 4 # wood\n",
    "        mask[mask >= 5] = 0 # resto: background\n",
    "\n",
    "        # Back to PILMask\n",
    "        mask = PILMask.create(mask)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da4cae-fd44-48d3-bc50-a5d259ccb3ac",
   "metadata": {},
   "source": [
    "While training, we will use the EarlyStopping strategy: after five epoch without improvements, the training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d61604-0aab-4708-bc91-08bbc983dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc = EarlyStoppingCallback(patience = 5, min_delta = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c9972-d7ed-49c4-9f2f-ccd00586a750",
   "metadata": {},
   "source": [
    "Then, we define all the hparams (hyperparameters) to build the datablocks, dataloaders and learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3e481-acc7-439b-ad6a-632b6e5115a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_hparams = {\n",
    "    \"blocks\": (ImageBlock, MaskBlock(codes)),\n",
    "    \"get_items\": get_image_files,\n",
    "    \"get_y\": get_y_fn,\n",
    "    \"splitter\": RandomSplitter(valid_pct=0.2),\n",
    "    \"item_tfms\": [Resize((480,640)), TargetMaskConvertTransform(), transformPipeline],\n",
    "    \"batch_tfms\": Normalize.from_stats(*imagenet_stats)\n",
    "}\n",
    "\n",
    "dls_hparams = {\n",
    "    \"source\": path_images,\n",
    "    \"bs\": 16\n",
    "}\n",
    "\n",
    "technique = KFold(n_splits = 5)\n",
    "\n",
    "learner_hparams = {\n",
    "    \"arch\": resnet18,\n",
    "    \"pretrained\": True,\n",
    "    \"metrics\": [DiceMulti()],\n",
    "    \"cbs\": []\n",
    "}\n",
    "\n",
    "learning_hparams = {\n",
    "    \"epochs\": 30,\n",
    "    \"base_lr\": 0.001,\n",
    "    \"freeze_epochs\": 3\n",
    "}\n",
    "\n",
    "learning_mode = \"finetune\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ad45c-05a0-40bf-8ea7-013e1dcbb8cb",
   "metadata": {},
   "source": [
    "In order to know how does this technique work, we can show an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456e419-4bfc-43d7-be80-b94f74c3d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruescog/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/torch/_tensor.py:1121: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "Exception occured in `CarveMix` when calling event `before_batch`:\n\tname 'HoleMakerBounded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     learn\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[1;32m      8\u001b[0m     learn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbefore_batch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m _, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[1;32m     12\u001b[0m dls\u001b[38;5;241m.\u001b[39mshow_batch(b \u001b[38;5;241m=\u001b[39m (cv\u001b[38;5;241m.\u001b[39mx, cv\u001b[38;5;241m.\u001b[39my), ctxs \u001b[38;5;241m=\u001b[39m axs\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastai/learner.py:157\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name): \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastcore/foundation.py:155\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, gen, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, \u001b[38;5;241m*\u001b[39margs, gen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[43mmap_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastcore/basics.py:790\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(g, iterable)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastcore/basics.py:775\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpop(v\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m    774\u001b[0m fargs \u001b[38;5;241m=\u001b[39m [args[x\u001b[38;5;241m.\u001b[39mi] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _Arg) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpargs] \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastai/learner.py:161\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, event_name): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39msorted(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastai/callback/core.py:65\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m, event_name)()\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#Reset self.run to True at each end of fit\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/fastai/callback/core.py:63\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     61\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mand\u001b[39;00m _run:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: res \u001b[38;5;241m=\u001b[39m \u001b[43mgetcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m modify_exception(e, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException occured in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` when calling event `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/visionmodelsevaluation/lib/python3.10/site-packages/semantic_segmentation_augmentations/carvemix.py:43\u001b[0m, in \u001b[0;36mCarveMix.before_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m sub_image, sub_mask \u001b[38;5;241m=\u001b[39m other_image[:, other_yhole, other_xhole], other_mask[other_yhole, other_xhole]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_position:\n\u001b[0;32m---> 43\u001b[0m     xhole, yhole \u001b[38;5;241m=\u001b[39m \u001b[43mHoleMakerBounded\u001b[49m(hole_size \u001b[38;5;241m=\u001b[39m sub_mask\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mget_hole(mask)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     xhole, yhole \u001b[38;5;241m=\u001b[39m other_xhole, other_yhole\n",
      "\u001b[0;31mNameError\u001b[0m: Exception occured in `CarveMix` when calling event `before_batch`:\n\tname 'HoleMakerBounded' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CarveMix(holes_num = 10, ROI_class = 1, delta_ratio = 1, random_position = True, p = 1)\n",
    "dls = DataBlock(**db_hparams).dataloaders(**dls_hparams)\n",
    "with Learner(dls, resnet18(), metrics=[DiceMulti()], cbs = cv) as learn:\n",
    "    learn.epoch, learn.training = 0, True\n",
    "    learn.dl = dls.train\n",
    "    b = dls.one_batch()\n",
    "    learn._split(b)\n",
    "    learn('before_train')\n",
    "    learn('before_batch')\n",
    "\n",
    "_, axs = plt.subplots(3, 3, figsize=(9, 9))\n",
    "dls.show_batch(b = (cv.x, cv.y), ctxs = axs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb0c4c-a99d-4ea8-83a6-2ac1e2fc0dfc",
   "metadata": {},
   "source": [
    "Finally, we test the model with distinct hparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40aa71-cddf-4eba-866c-26ac9ff5e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for holes_num in [1]: #, 3, 5]:\n",
    "    for ROI_class in [1]: #, 1]:\n",
    "        cm = CarveMix(holes_num, ROI_class)\n",
    "        learner_hparams[\"cbs\"] = [esc, cm]\n",
    "        r = evaluate(db_hparams,\n",
    "                     dls_hparams,\n",
    "                     technique,\n",
    "                     learner_hparams,\n",
    "                     learning_hparams,\n",
    "                     learning_mode\n",
    "                    )\n",
    "        results.update({\n",
    "            str(holes_num) + str(ROI_class): r[\"DiceMulti\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4615d6-e095-4d5b-8049-d94586232b10",
   "metadata": {},
   "source": [
    "And plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655d011-86b2-4f0c-a972-adede94188ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146564d1-d99a-4ea2-b94e-00d5273bbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df[df.columns]);\n",
    "plt.plot([i for i in range(1, len(df.columns) + 1)], df.describe().transpose()[\"std\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bce01f-21ae-4386-b8a0-da19e1117093",
   "metadata": {},
   "source": [
    "We will show its mean and standar deviation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca18e8-e6c1-4221-8489-4e173b30bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()[[\"mean\", \"std\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-visionmodelsevaluation]",
   "language": "python",
   "name": "conda-env-.conda-visionmodelsevaluation-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
